{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djGTEXK3AJ59",
        "outputId": "743dde1a-13f1-4cb1-f389-2384a7c9a019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: bs4 in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from bs4) (4.14.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4IXfg7bBXjD",
        "outputId": "0ed9ea22-d899-4aa4-fa07-76f2686669a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Skipping (exists): data/indian_ocean/2025/01/20250101_prof.nc\n",
            "✅ Skipping (exists): data/indian_ocean/2025/01/20250102_prof.nc\n",
            "✅ Skipping (exists): data/indian_ocean/2025/01/20250103_prof.nc\n",
            "✅ Skipping (exists): data/indian_ocean/2025/01/20250104_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250105_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250105_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250106_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250106_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250107_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250107_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250108_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250108_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250109_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250109_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250110_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250110_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250111_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250111_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250112_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250112_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250113_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250113_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250114_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250114_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250115_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250115_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250116_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250116_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250117_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250117_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250118_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250118_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250119_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250119_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250120_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250120_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250121_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250121_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250122_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250122_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250123_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250123_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250124_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250124_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250125_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250125_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250126_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250126_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250127_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250127_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250128_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250128_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250129_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250129_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250130_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250130_prof.nc\n",
            "⬇️ Downloading: https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/20250131_prof.nc\n",
            "✅ Saved: data/indian_ocean/2025/01/20250131_prof.nc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Base URL\n",
        "BASE_URL = \"https://data-argo.ifremer.fr/geo/indian_ocean/2025/01/\"\n",
        "LOCAL_DIR = \"data/indian_ocean/2025/01/\"\n",
        "\n",
        "# Ensure local directory exists\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "def get_nc_files(url):\n",
        "    \"\"\"Scrape the directory listing to get .nc file URLs\"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.nc')]\n",
        "    return [urljoin(url, link) for link in links]\n",
        "\n",
        "def download_file(file_url, local_path):\n",
        "    \"\"\"Download a file if not already present\"\"\"\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"✅ Skipping (exists): {local_path}\")\n",
        "        return\n",
        "    print(f\"⬇️ Downloading: {file_url}\")\n",
        "    r = requests.get(file_url, stream=True)\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(f\"✅ Saved: {local_path}\")\n",
        "\n",
        "def main():\n",
        "    file_urls = get_nc_files(BASE_URL)\n",
        "    for file_url in file_urls:\n",
        "        filename = os.path.basename(file_url)\n",
        "        local_path = os.path.join(LOCAL_DIR, filename)\n",
        "        download_file(file_url, local_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4438bc9",
        "outputId": "694082aa-26b1-4326-c1c8-1d18de4d1040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of N_PROF and N_LEVELS for each file:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'dimensions_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummary of N_PROF and N_LEVELS for each file:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdimensions_data\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_PROF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[33m'\u001b[39m\u001b[33mN_PROF\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_LEVELS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[33m'\u001b[39m\u001b[33mN_LEVELS\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'dimensions_data' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Summary of N_PROF and N_LEVELS for each file:\")\n",
        "for entry in dimensions_data:\n",
        "    print(f\"File: {entry['filename']}, N_PROF: {entry['N_PROF']}, N_LEVELS: {entry['N_LEVELS']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9ebd12"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the extracted N_PROF and N_LEVELS information for all files in the directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9191e6a9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A total of 31 NetCDF files were identified in the directory `/content/data/indian_ocean/2025/01/`.\n",
        "*   For each of these files, the `N_PROF` and `N_LEVELS` dimension values were successfully extracted using `xarray`.\n",
        "*   The extraction process was refined to use `ds.sizes.get()` for retrieving dimension lengths, which addressed `FutureWarning` messages related to `Dataset.dims` and ensures compatibility with future `xarray` versions.\n",
        "*   The `N_PROF` and `N_LEVELS` values varied across files; for example:\n",
        "    *   `20250118_prof.nc` had `N_PROF`: 68 and `N_LEVELS`: 1165.\n",
        "    *   `20250125_prof.nc` had `N_PROF`: 89 and `N_LEVELS`: 1218.\n",
        "    *   `20250101_prof.nc` had `N_PROF`: 77 and `N_LEVELS`: 1334.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   This summary provides a foundational understanding of the profile and level counts within each NetCDF file, which is crucial for subsequent data processing and analysis, such as ensuring consistent data structures or identifying outliers.\n",
        "*   Further analysis could involve calculating descriptive statistics (e.g., minimum, maximum, average, standard deviation) for `N_PROF` and `N_LEVELS` across all files to understand the overall characteristics and variability of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USPY4sabp84H",
        "outputId": "27ed6687-8302-4034-cdbd-d829c00b9f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average N_PROF across all files: 78.52\n",
            "Average N_LEVELS across all files: 1385.55\n"
          ]
        }
      ],
      "source": [
        "total_n_prof = sum([entry['N_PROF'] for entry in dimensions_data])\n",
        "total_n_levels = sum([entry['N_LEVELS'] for entry in dimensions_data])\n",
        "num_files = len(dimensions_data)\n",
        "\n",
        "average_n_prof = total_n_prof / num_files\n",
        "average_n_levels = total_n_levels / num_files\n",
        "\n",
        "print(f\"Average N_PROF across all files: {average_n_prof:.2f}\")\n",
        "print(f\"Average N_LEVELS across all files: {average_n_levels:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3u2kmP-Fd_7",
        "outputId": "731893de-1133-4433-f999-575302ff707a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extracted 77 profiles and 49620 measurements.\n",
            "\n",
            "--- Profiles Table ---\n",
            "   profile_id  float_id  cycle_number            datetime   latitude  \\\n",
            "0           0  b5906527            94 2025-01-01 23:10:46 -43.513000   \n",
            "1           1  b5905521            96 2025-01-01 22:30:06  -8.248500   \n",
            "2           2  b5906256           168 2025-01-01 22:23:17 -43.034000   \n",
            "3           3  b1902259           164 2025-01-01 22:06:27 -33.786720   \n",
            "4           4  b7900576           146 2025-01-01 21:39:20 -45.424212   \n",
            "\n",
            "   longitude project_name data_mode platform_type institution  \\\n",
            "0   35.46800         ARGO         R       unknown     FR GDAC   \n",
            "1  105.79380         ARGO         R       unknown     FR GDAC   \n",
            "2  134.93100         ARGO         R       unknown     FR GDAC   \n",
            "3   55.46375         ARGO         R       unknown     FR GDAC   \n",
            "4   43.18347         ARGO         R       unknown     FR GDAC   \n",
            "\n",
            "                         title      source  min_pressure  max_pressure  \\\n",
            "0  Argo float vertical profile  Argo float          4.40   1598.229980   \n",
            "1  Argo float vertical profile  Argo float          4.10   2003.699951   \n",
            "2  Argo float vertical profile  Argo float          4.40   1997.939941   \n",
            "3  Argo float vertical profile  Argo float          0.96   2005.239990   \n",
            "4  Argo float vertical profile  Argo float          5.20   1580.900024   \n",
            "\n",
            "                                 measured_parameters  \n",
            "0  DATA_TYPE, FORMAT_VERSION, HANDBOOK_VERSION, R...  \n",
            "1  DATA_TYPE, FORMAT_VERSION, HANDBOOK_VERSION, R...  \n",
            "2  DATA_TYPE, FORMAT_VERSION, HANDBOOK_VERSION, R...  \n",
            "3  DATA_TYPE, FORMAT_VERSION, HANDBOOK_VERSION, R...  \n",
            "4  DATA_TYPE, FORMAT_VERSION, HANDBOOK_VERSION, R...  \n",
            "\n",
            "--- Measurements Table ---\n",
            "   profile_id  level  pressure  temperature   salinity\n",
            "0           0      0       4.4       10.852  34.022999\n",
            "1           0      1       6.0       10.852  34.022999\n",
            "2           0      2       8.0       10.852  34.022999\n",
            "3           0      3      10.0       10.851  34.022999\n",
            "4           0      4      12.0       10.852  34.022999\n"
          ]
        }
      ],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extract_profiles(nc_file):\n",
        "    ds = xr.open_dataset(nc_file)\n",
        "\n",
        "    # --- Identify dimensions safely ---\n",
        "    n_prof = ds.sizes.get(\"N_PROF\", 0)\n",
        "    n_levels = ds.sizes.get(\"N_LEVELS\", 0)\n",
        "    if n_prof == 0 or n_levels == 0:\n",
        "        print(f\"⚠️ Skipping file (no valid dimensions): {nc_file}\")\n",
        "        ds.close()\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # --- Extract core variables ---\n",
        "    pres = ds[\"PRES\"].values if \"PRES\" in ds.variables else np.full((n_prof, n_levels), np.nan)\n",
        "    temp = ds[\"TEMP\"].values if \"TEMP\" in ds.variables else np.full((n_prof, n_levels), np.nan)\n",
        "    psal = ds[\"PSAL\"].values if \"PSAL\" in ds.variables else np.full((n_prof, n_levels), np.nan)\n",
        "    lat = ds[\"LATITUDE\"].values if \"LATITUDE\" in ds.variables else np.full(n_prof, np.nan)\n",
        "    lon = ds[\"LONGITUDE\"].values if \"LONGITUDE\" in ds.variables else np.full(n_prof, np.nan)\n",
        "\n",
        "    # --- Build measurements table (normalized) ---\n",
        "    measurements_records = []\n",
        "    for i in range(n_prof):\n",
        "        for j in range(n_levels):\n",
        "            # Skip empty measurements\n",
        "            if np.isnan(pres[i, j]) or np.isnan(temp[i, j]) or np.isnan(psal[i, j]):\n",
        "                continue\n",
        "            measurements_records.append({\n",
        "                \"profile_id\": i,  # local profile index within file\n",
        "                \"level\": j,\n",
        "                \"latitude\": float(lat[i]) if not np.isnan(lat[i]) else None,\n",
        "            \"longitude\": float(lon[i]) if not np.isnan(lon[i]) else None,\n",
        "                \"pressure\": float(pres[i, j]),\n",
        "                \"temperature\": float(temp[i, j]),\n",
        "                \"salinity\": float(psal[i, j])\n",
        "            })\n",
        "\n",
        "    measurements_df = pd.DataFrame(measurements_records)\n",
        "\n",
        "    # --- Build profiles table (metadata) ---\n",
        "    profiles_records = []\n",
        "    for i in range(n_prof):\n",
        "        # Try to get datetime safely\n",
        "        try:\n",
        "            datetime_val = pd.to_datetime(ds[\"JULD\"][i].values)\n",
        "        except Exception:\n",
        "            datetime_val = None\n",
        "\n",
        "        # Handle float_id safely (can be array of chars)\n",
        "        if \"PLATFORM_NUMBER\" in ds.variables:\n",
        "            float_id = str(ds[\"PLATFORM_NUMBER\"][i].values)\n",
        "            float_id = float_id.strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
        "        else:\n",
        "            float_id = \"unknown\"\n",
        "\n",
        "        record = {\n",
        "            \"profile_id\": i,\n",
        "            \"float_id\": float_id,\n",
        "            \"cycle_number\": int(ds[\"CYCLE_NUMBER\"][i].values) if \"CYCLE_NUMBER\" in ds.variables else None,\n",
        "            \"datetime\": datetime_val,\n",
        "            \"latitude\": float(lat[i]) if not np.isnan(lat[i]) else None,\n",
        "            \"longitude\": float(lon[i]) if not np.isnan(lon[i]) else None,\n",
        "            \"project_name\": str(ds.attrs.get(\"project_name\", \"ARGO\")),\n",
        "            \"data_mode\": str(ds.attrs.get(\"data_mode\", \"R\")),\n",
        "            \"platform_type\": str(ds.attrs.get(\"platform_type\", \"unknown\")),\n",
        "            \"institution\": str(ds.attrs.get(\"institution\", \"unknown\")),\n",
        "            \"title\": str(ds.attrs.get(\"title\", \"unknown\")),\n",
        "            \"source\": str(ds.attrs.get(\"source\", \"unknown\")),\n",
        "            \"min_pressure\": float(np.nanmin(pres[i])) if np.any(~np.isnan(pres[i])) else None,\n",
        "            \"max_pressure\": float(np.nanmax(pres[i])) if np.any(~np.isnan(pres[i])) else None,\n",
        "            \"measured_parameters\": \", \".join([v for v in ds.data_vars if v not in [\"N_PROF\", \"N_LEVELS\"]])\n",
        "        }\n",
        "        profiles_records.append(record)\n",
        "\n",
        "    profiles_df = pd.DataFrame(profiles_records)\n",
        "\n",
        "    ds.close()\n",
        "\n",
        "    print(f\"✅ Extracted {len(profiles_df)} profiles and {len(measurements_df)} measurements.\")\n",
        "    return profiles_df, measurements_df\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    nc_file = \"/content/data/indian_ocean/2025/01/20250101_prof.nc\"\n",
        "    profiles_df, measurements_df = extract_profiles(nc_file)\n",
        "\n",
        "    print(\"\\n--- Profiles Table ---\")\n",
        "    print(profiles_df.head())\n",
        "\n",
        "    print(\"\\n--- Measurements Table ---\")\n",
        "    print(measurements_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoHxQ3FGTZi"
      },
      "source": [
        "## profiles_df → Vector DB\n",
        "\n",
        "This DataFrame has all the contextual metadata about each profile:\n",
        "\n",
        "float_id, cycle_number, datetime, latitude, longitude,\n",
        "project_name, data_mode, platform_type, institution, title,\n",
        "source, measured_parameters, min_pressure, max_pressure\n",
        "\n",
        "\n",
        "LLMs can use this to answer questions like:\n",
        "\n",
        "“Which Argo floats were deployed by INCOIS in January 2025?”\n",
        "“Find the float that measured deepest salinity profiles near equator.”\n",
        "\n",
        "These don’t need precise numeric filtering — instead, they’re semantic / contextual.\n",
        "\n",
        "## measurements_df → PostgreSQL\n",
        "\n",
        "Contains the actual depth-wise numeric data:\n",
        "\n",
        "profile_id, level, pressure, temperature, salinity\n",
        "\n",
        "\n",
        "This is what your system queries when the user asks:\n",
        "\n",
        "“What’s the temperature 30 m off the coast of Tamil Nadu?”\n",
        "“Show me salinity at 50 dbar for profile 102.”\n",
        "\n",
        "PostgreSQL can handle these fast and precisely using indexes on pressure, latitude, longitude."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
